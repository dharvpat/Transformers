{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras import layers\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0, epsilon=1e-6, attention_axes=None, kernel_size=1):\n",
    "  \"\"\"\n",
    "  Creates a single transformer block.\n",
    "  \"\"\"\n",
    "  x = layers.LayerNormalization(epsilon=epsilon)(inputs)\n",
    "  x = layers.MultiHeadAttention(\n",
    "      key_dim=head_size, num_heads=num_heads, dropout=dropout,\n",
    "      attention_axes=attention_axes\n",
    "      )(x, x)\n",
    "  x = layers.Dropout(dropout)(x)\n",
    "  res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "  x = layers.LayerNormalization(epsilon=epsilon)(res)\n",
    "  x = layers.Conv1D(filters=ff_dim, kernel_size=kernel_size, activation=\"relu\")(x)\n",
    "  x = layers.Dropout(dropout)(x)\n",
    "  x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=kernel_size)(x)\n",
    "  return x + res\n",
    "\n",
    "def build_transfromer(head_size, num_heads, ff_dim, num_trans_blocks, mlp_units, dropout=0, mlp_dropout=0, attention_axes=None, epsilon=1e-6, kernel_size=1):\n",
    "  \"\"\"\n",
    "  Creates final model by building many transformer blocks.\n",
    "  \"\"\"\n",
    "  n_timesteps, n_features, n_outputs = 15, 1, 5\n",
    "  inputs = tf.keras.Input(shape=(n_timesteps, n_features))\n",
    "  x = inputs\n",
    "  for _ in range(num_trans_blocks):\n",
    "    x = transformer_encoder(x, head_size=head_size, num_heads=num_heads, ff_dim=ff_dim, dropout=dropout, attention_axes=attention_axes, kernel_size=kernel_size, epsilon=epsilon)\n",
    "\n",
    "  x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "  for dim in mlp_units:\n",
    "    x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "  outputs = layers.Dense(n_outputs)(x)\n",
    "  return tf.keras.Model(inputs, outputs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
